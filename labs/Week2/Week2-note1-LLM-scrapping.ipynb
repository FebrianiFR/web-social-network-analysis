{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8d6be-94d0-4d2d-b5b7-bb9706c3b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c6239-545d-4d76-931f-6921a73f6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72036e4c-4c46-4449-bf05-ee43d314b70e",
   "metadata": {},
   "source": [
    "Gemini provide free API for light use, see https://ai.google.dev/pricing for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab90fc4-24a1-464b-9f9c-e1bada90c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Gemini API key\n",
    "# PLEASE GET YOUR OWN GEMINI API via https://ai.google.dev/\n",
    "GEMINI_API_KEY = \"YOUR OWN API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f7505-f1b2-4e42-959f-4d36c96f7603",
   "metadata": {},
   "source": [
    "# Simple Task: extract info from static website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6253767-6af5-47e7-95a9-a9cc212e3c8e",
   "metadata": {},
   "source": [
    "Step 1: \n",
    "- scrape content using `bs4`\n",
    "- parse to html\n",
    "- apply simple fiiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb926f8-b044-4a0b-917e-285f7411672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(url):\n",
    "    \"\"\"\n",
    "    Scrape main content from a webpage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            tag.decompose()\n",
    "            \n",
    "        # Extract main content (customize based on website structure)\n",
    "        content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "        \n",
    "        if content:\n",
    "            return ' '.join(content.stripped_strings)\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26ce5e-afa1-4d27-b4ff-94765d3aa8d1",
   "metadata": {},
   "source": [
    "Step 2: use LLM to get a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf9d9d-41f2-4331-8a9f-1033f4ae9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_with_gemini(text, api_key):\n",
    "    \"\"\"\n",
    "    Generate summary using Gemini API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure the Gemini API\n",
    "        genai.configure(api_key=api_key)\n",
    "        \n",
    "        # Initialize the model (using the most capable model available)\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        \n",
    "        # Prepare the prompt\n",
    "        prompt = f\"\"\"Please provide a concise summary of the following text in 2-3 sentences:\n",
    "        \n",
    "        {text[:30000]}\"\"\"  # Gemini has a higher token limit than GPT-3.5\n",
    "        \n",
    "        # Generate response\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Gemini API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971ccb9-e905-49ea-be57-fde1016354ae",
   "metadata": {},
   "source": [
    "Step 3: \n",
    "- Type the website url and apply the code above to get info\n",
    "- Summarise them into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641dc7f6-d499-4dc1-b2eb-9b0cd3c4a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_urls(urls, api_key):\n",
    "    \"\"\"\n",
    "    Process multiple URLs and create a summary table\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"Processing: {url}\")\n",
    "        content = scrape_content(url)\n",
    "        \n",
    "        if content:\n",
    "            summary = summarize_with_gemini(content, api_key)\n",
    "            \n",
    "            results.append({\n",
    "                'URL': url,\n",
    "                'Summary': summary,\n",
    "                'Content Length': len(content),\n",
    "                'Timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "            \n",
    "            # Respect API rate limits\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('web_summaries.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca07848-4845-4404-bb36-6c61b4eac054",
   "metadata": {},
   "source": [
    "Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb454bc-8474-475e-a33c-2cbe5fa10fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to process\n",
    "urls_to_process = [\n",
    "    \"http://www.drps.ed.ac.uk/24-25/dpt/cxcmse11615.htm\",\n",
    "    \"http://www.drps.ed.ac.uk/24-25/dpt/cxcmse11427.htm\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd0d30-c99e-40f2-863b-bc2cd99cec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process URLs and get results\n",
    "summary_table = process_urls(urls_to_process, GEMINI_API_KEY)\n",
    "print(\"\\nSummary Table:\")\n",
    "print(summary_table.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa14f81-301f-40e1-ac80-2f1f2c9b33cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca975960-4440-4e49-9629-7f631cf42d92",
   "metadata": {},
   "source": [
    "# Scraping dynamic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c834f-26a1-4c6f-8727-92fef76b0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aaea6d-462a-4931-9ff9-c6760ca9f88e",
   "metadata": {},
   "source": [
    "* Step 1: Get product listings\n",
    "* Step 2: Summarise description in a short sentence\n",
    "* Step 3: Process the steps above on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e587b2ba-9c14-4db0-9249-4e110ef29ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's define a class to do this task\n",
    "\n",
    "!!! Please locate the website and check if other selectors or PATH can be used to find the corresponding info\n",
    "\"\"\"\n",
    "\n",
    "class GitzScraper:\n",
    "    def __init__(self, api_key):\n",
    "        self.base_url = \"https://gitz.bz\"\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Configure GenAI if you want to summarise reviews\n",
    "        genai.configure(api_key=self.api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-pro')\n",
    "        \n",
    "        # Optional headers to mimic a normal browser visit\n",
    "        self.headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def random_delay(self):\n",
    "        \"\"\"Introduce a random delay to avoid hitting the server too quickly.\"\"\"\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"\n",
    "        Fetch HTML content from a URL using Requests and\n",
    "        return a BeautifulSoup object.\n",
    "        \"\"\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        response.raise_for_status()  # Raise an error for bad statuses\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    def get_product_listings(self, url):\n",
    "        \"\"\"\n",
    "        Fetch product listings from the given page on gitz.bz.\n",
    "        Please inspect the website and use suitable way to locate the pages!\n",
    "        \"\"\"\n",
    "        products = []\n",
    "        try:\n",
    "            print(f\"Accessing: {url}\")\n",
    "            soup = self.get_soup(url)\n",
    "\n",
    "            # Step 1: Get the product cards\n",
    "            # Locate all the product cards by <li class=\"productgrid--item\">\n",
    "            product_cards = soup.find_all('li', class_='productgrid--item')\n",
    "            print(f\"Found {len(product_cards)} product cards.\")\n",
    "\n",
    "            for card in product_cards:\n",
    "                try:\n",
    "                    # Product name & link\n",
    "                    # e.g. <h2 class=\"productitem--title\"><a href=\"...\">PRODUCT NAME</a></h2>\n",
    "                    name_container = card.find('h2', class_='productitem--title')\n",
    "                    if name_container:\n",
    "                        link_tag = name_container.find('a')\n",
    "                        product_name = link_tag.get_text(strip=True) if link_tag else \"N/A\"\n",
    "                        product_url = urljoin(self.base_url, link_tag.get(\"href\")) if link_tag else None\n",
    "                    else:\n",
    "                        product_name = \"N/A\"\n",
    "                        product_url = None\n",
    "    \n",
    "                    # Price: \n",
    "                    # e.g. <div class=\"price--main\"><span class=\"money\">$653.60 BZD</span></div>\n",
    "                    price_main = card.find('div', class_='price--main')\n",
    "                    if price_main:\n",
    "                        price_span = price_main.find('span', class_='money')\n",
    "                        price = price_span.get_text(strip=True) if price_span else \"N/A\"\n",
    "                    else:\n",
    "                        price = \"N/A\"\n",
    "    \n",
    "                    # Step 2: Now fetch the product detail page for the full description\n",
    "                    #         (class=\"description\" as you mentioned).\n",
    "                    if product_url:\n",
    "                        description = self.get_product_description(product_url)\n",
    "\n",
    "                        \"\"\"\n",
    "                        Using LLM to summarise the description\n",
    "                        \"\"\" \n",
    "                        summary =  self.summarize_with_gemini(description, self.api_key)\n",
    "                    else:\n",
    "                        summary = \"N/A\"\n",
    "    \n",
    "                    products.append({\n",
    "                        'Name': product_name,\n",
    "                        'Price': price,\n",
    "                        'Product URL': product_url,\n",
    "                        'Description': summary\n",
    "                    })\n",
    "                    print(f\"Found product: {product_name}\")\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing a product card: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching product listings: {str(e)}\")\n",
    "    \n",
    "        return products\n",
    "\n",
    "    def get_product_description(self, product_url):\n",
    "        \"\"\"\n",
    "        Given a product URL, fetch the detail page and extract\n",
    "        the product description from the <div class=\"description\"> (or whichever).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If your product detail pages require this same get_soup approach\n",
    "            soup = self.get_soup(product_url)\n",
    "    \n",
    "            desc_div = soup.find('div', class_='product-description rte')\n",
    "            if desc_div:\n",
    "                # If the description text is nested further, you may need find() or get_text()\n",
    "                return desc_div.get_text(strip=True)\n",
    "            else:\n",
    "                return \"N/A\"\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching product description: {str(e)}\")\n",
    "            return \"N/A\"\n",
    "        \n",
    "    def process_search_results(self, search_url):\n",
    "        \"\"\"\n",
    "        Main method: fetch product data from a search (or listing) page,\n",
    "        for each product, summarise them,\n",
    "        and save the results to a CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            products = self.get_product_listings(search_url)\n",
    "            if not products:\n",
    "                print(\"No products found.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            results = []\n",
    "            for product in products:\n",
    "                print(f\"Processing product: {product['Name']}\")\n",
    "                self.random_delay()  # small delay\n",
    "\n",
    "                results.append({\n",
    "                    \"Product Name\": product[\"Name\"],\n",
    "                    \"Price\": product[\"Price\"],\n",
    "                    'Description': product[\"Description\"]\n",
    "                })\n",
    "\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(\"gitz_bz_product_analysis.csv\", index=False)\n",
    "            print(\"Results saved to gitz_bz_product_analysis.csv\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing search results: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def summarize_with_gemini(self, text, api_key):\n",
    "        \"\"\"\n",
    "        Generate summary using Gemini API\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Configure the Gemini API\n",
    "            genai.configure(api_key=api_key)\n",
    "            \n",
    "            # Initialize the model (using the most capable model available)\n",
    "            model = genai.GenerativeModel('gemini-pro')\n",
    "            \n",
    "            # Prepare the prompt\n",
    "            prompt = f\"\"\"Please provide a concise summary of the following text in one sentence:\n",
    "            \n",
    "            {text[:30000]}\"\"\"  # Gemini has a higher token limit than GPT-3.5\n",
    "            \n",
    "            # Generate response\n",
    "            response = model.generate_content(prompt)\n",
    "            \n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error with Gemini API: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105e50e-8396-47cb-ba48-799d4335951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = GitzScraper(GEMINI_API_KEY)\n",
    "\n",
    "search_url = \"https://gitz.bz/search?type=product&q=wood*+desk*\"\n",
    "\n",
    "results = scraper.process_search_results(search_url)\n",
    "\n",
    "if not results.empty:\n",
    "    print(\"\\nProduct Analysis Results:\")\n",
    "    print(results)\n",
    "else:\n",
    "    print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5e051-6ecf-4b6f-8f93-70180526d8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "690e21df-ca8a-441e-91f1-d58fc45776d2",
   "metadata": {},
   "source": [
    "# Web Scraping with Third-Party Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2460d78d-02a2-4503-bb75-8baad6028e13",
   "metadata": {},
   "source": [
    "For example,**BuildShip - Website Scraping Playground**, https://llm-web-crawler.vercel.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39c07b-7037-482c-a46a-de655220df0c",
   "metadata": {},
   "source": [
    "For example, in the above website\n",
    "- `Select Scrape Mode`: LLM Extraction\n",
    "- `URL`: https://gitz.bz/search?type=product&q=desk*\n",
    "- `Selector`: .productgrid--wrapper\n",
    "- `Extraction Fields`: title, price, ratings, description\n",
    "- `Extraction Mode`: HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c769e9-967c-422a-9f95-be83204292cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
